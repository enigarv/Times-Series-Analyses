# -*- coding: utf-8 -*-
"""TimeSeries_forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eOvLhcAHpPpffJEwIvv939G1TGgqRShy

# **Time Series Forecasting**
"""

from google.colab import drive

drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pmdarima
# !pip install statsmodels --upgrade

# Import packages
import numpy as np
import pandas as pd
import math
import holidays
from math import log, exp
import itertools
from datetime import datetime
from matplotlib import pyplot as plt
from scipy.interpolate import interp1d
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import kpss
from pandas.util._decorators import Appender
from statsmodels.compat.pandas import Appender
import statsmodels.api as sm
import seaborn as sns

# ARIMA and UCM packages
from statsmodels.tsa.deterministic import DeterministicProcess
from statsmodels.tsa.arima.model import ARIMA
import scipy.stats
from statsmodels.tsa.statespace.sarimax import SARIMAX
from scipy import stats
from statsmodels.tsa.arima.model import ARIMA
import scipy.stats
from statsmodels.tsa.statespace.sarimax import SARIMAX
from scipy import stats
from scipy.stats import normaltest
import warnings
warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',
                        FutureWarning)
warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',
                        FutureWarning)

# Machine Learning Packages
from tensorflow import keras
from sklearn.preprocessing import MinMaxScaler
from keras.utils.vis_utils import plot_model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Bidirectional, GRU, Dense, TimeDistributed 
from sklearn.metrics import mean_squared_error

PATH = '/content/gdrive/MyDrive/TimeSeries/'

data = pd.read_csv(PATH + 'train.csv')

data.head()

"""## <font color='#264653'><b>Definition of functions and preliminary processing operations

There follow a series of functions necessary for data processing and for evaluating the goodness of the models.
"""

# Funzione per il calcolo del MAPE
def mape(y_true, y_pred):
    mape = np.mean(abs((y_true-y_pred)/y_true))*100
    print('MAPE: %.2f'% (mape), "%") 
    return mape

# Pre-processing
def date_time(data):
  """
  Function for merging Date and Hour columns into one single column
  called 'dtime'
  """
  data['dtime'] = 0
  for i in range(len(data)):
      date_time = data.Date[i] + ' ' + str(data.Hour[i]) + ':00:00'
      datetime_obj = datetime.strptime(date_time, '%Y-%m-%d %H:%M:%S')
      data.dtime[i] = datetime_obj   
  data = data.drop(labels = 'Date', axis=1)
  data = data.drop(labels = 'Hour', axis=1)
  
  return data

def interpolation(data):
  """
  Function for managing missing data, NA, through interpolation
  """
  data['rownum'] = np.arange(data.shape[0])
  df_nona = data.dropna(subset = ['CO'])
  f = interp1d(df_nona['rownum'], df_nona['CO'])
  data['co'] = f(data['rownum'])
  data = data.drop(labels = 'rownum', axis=1)
  data = data.drop(labels = 'CO', axis=1)
  
  return data

def transform(serie, transformation):
  """
  Function that performs the type of transformation passed as input to the Series
  :param pd.Series serie: time series
  :parama string transformation: string specifying the type of transformation
                                 to be done between 'log' and 'exp'
  """
  new_serie = []
  for i in range(len(serie)):
    if transformation == 'log':
      new_serie.append(log(serie[i]))
    else: 
      new_serie.append(exp(serie[i]))

  return pd.Series(new_serie)

# Differenziazione
def difference(serie, interval = 1):
	"""
  Function that performs data differentiation according to a certain number of intervals
  :param pd.Series serie: times series
  :parama int interval: number of differentiations
  """
	diff = []
	for i in range(interval, len(serie)):
		value = serie[i] - serie[i - interval]
		diff.append(value)
	return pd.Series(diff)

# Pre-processing
data = date_time(data)
data = interpolation(data)
# Set datetime as index
tdi = pd.DatetimeIndex(data.dtime)
data = data.set_index(tdi, drop = True)        
data = data.drop(labels = 'dtime', axis=1)

# Data split in train and test
train = data.loc[:'2005-02-12 23:00:00']
test = data.loc['2005-02-13 00:00:00':'2005-02-28 23:00:00']

df = data.copy()
# Preparation of the dataset for the study of seasonality
# by adding some useful variables
df['year'] = [d.year for d in df.index]
df['month'] = [d.strftime('%b') for d in df.index]
df['day']= [d.strftime('%d') for d in df.index]
df['hour'] = [d.strftime('%H') for d in df.index]
df['week'] = [d.strftime("%V") for d in df.index]
df['day_week'] = df.index.day_name()
months = df['month'].unique()
days = df['day'].unique()

# Regressor 'weekend'
df['weekend'] = df['day_week'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)
# Regressor 'holiday'
festivi = list(holidays.Italy(years=[2004, 2005]).items())
df['date'] = list(df.index)
df['holiday'] = df.date.apply(lambda x: 1 if x in festivi else 0)

df = df.drop('date', axis = 1)

def forecast_df(start, end):
    """
    Function for creating the dataset that will contain the forecasts of the 
    three approaches
    """
    date = pd.date_range(start = start, end = end).to_pydatetime().tolist()
    hour = list(np.arange(0,24))

    forecast_date = []
    for i in range(len(date)):
        for j in hour:
            forecast_date.append(date[i].replace(hour = j))
    forecast_period = pd.DataFrame(columns = ['dtime'])
    forecast_period.dtime = forecast_date
    
    tdi = pd.DatetimeIndex(forecast_period.dtime)
    forecast_dataset = forecast_period.set_index(tdi, drop = True)
    forecast_dataset = forecast_period.drop(labels = 'dtime', axis = 1)
    
    return forecast_dataset

# Creation of datasets that will collect forecasts
start = '2005-03-01 00:00:00'
end = '2005-03-31 23:00:00'
fc_df = forecast_df(start, end)

# Forecast dataset with useful
fc_data = forecast_df(start, end)
# Set datetime as index
tdi = pd.DatetimeIndex(fc_data.dtime)
fc_data = fc_data.set_index(tdi, drop = True)        
fc_data = fc_data.drop(labels = 'dtime', axis=1)

# 
fc_data['year'] = [d.year for d in fc_data.index]
fc_data['month'] = [d.strftime('%b') for d in fc_data.index]
fc_data['day']= [d.strftime('%d') for d in fc_data.index]
fc_data['hour'] = [d.strftime('%H') for d in fc_data.index]
fc_data['week'] = [d.strftime("%V") for d in fc_data.index]
fc_data['day_week'] = fc_data.index.day_name()
months = fc_data['month'].unique()
days = fc_data['day'].unique()

# Regressor weekend
fc_data['weekend'] = fc_data['day_week'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)
# Regressor holiday
festivi = list(holidays.Italy(years=[2004, 2005]).items())
fc_data['date'] = list(fc_data.index)
fc_data['holiday'] = fc_data.date.apply(lambda x: 1 if x in festivi else 0)

fc_data = fc_data.drop('date', axis = 1)

# Logarithm of the data to deal with non-stationarity
data['log_co'] = list(transform(data.co, 'log'))

train_log = data.log_co.loc[:'2005-02-12 23:00:00']
test_log = data.log_co.loc['2005-02-13 00:00:00':'2005-02-28 23:00:00']

"""## <font color='#2a9d8f'><b>Time Series Analyses

### Graphical visualization of data
"""

# Plot 
fig, axs = plt.subplots(2, 2, figsize=(11,11))

axs[0, 0].set_title('Andamento CO')
axs[0, 0].plot(data.co, lw = 0.5, color = 'steelblue')
# Daily
axs[0, 1].set_title('Andamento giornaliero CO')
axs[0, 1].plot(data.co[6:30], color = 'steelblue')
# Weekly
axs[1, 0].set_title('Andamento prima settimana CO')
axs[1, 0].plot(data.co[0:171])
# Two weeks 
axs[1, 1].set_title('Andamento prime due settimane CO')
axs[1, 1].plot(data.co[0:342])

for ax in axs.flat:
    ax.set(xlabel='DateTime', ylabel='CO')

"""### Stationarity"""

# Adf Test
def adf_test(timeseries, window = 12, cutoff = 0.01, plot = False):
    if plot == True:
      # Rolling Mean
      rolmean = timeseries.rolling(window).mean()

      #Plot rolling statistics
      fig = plt.figure(figsize=(12, 8))
      orig = plt.plot(timeseries, color='blue',label='Original')
      mean = plt.plot(rolmean, color='red', label='Rolling Mean')
      plt.legend(loc='best')
      plt.title('Rolling Mean') 
      plt.show()
    
    # Augmemnted Dickey-Fuller Test
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )
    output = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        output['Critical Value (%s)'%key] = value
    pvalue = dftest[1]
    if pvalue < cutoff:
        print('p-value = %.4f. The series is likely stationary.' % pvalue)
    else:
        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)
    
    print(output)

# Kpss Test
def kpss_test(series):    
    statistic, p_value, n_lags, critical_values = kpss(series)
    
    print(f'KPSS Statistic: {statistic}')
    print(f'p-value: {p_value}')
    print(f'num lags: {n_lags}')
    print('Critial Values:')
    for key, value in critical_values.items():
        print(f'   {key} : {value}')
    print(f'Result: The series is {"not " if p_value < 0.05 else ""}stationary')

adf_test(data['co'])

"""IThe *p-value* is very small, $5.284311 \cdot 10^{-12}$; 
in addition, the value of the test statistic is lower than the critical values. 

It follows that the series is **stationary** and does not require differentiation.
"""

kpss_test(data.co)

data['date'] = data.index.date
corr = data.pivot_table(index='date', values='co', aggfunc=['mean', 'std'])

plt.figure(figsize=(13, 5))
plt.plot(corr['mean'], corr['std'], 'o')
sns.regplot(x = 'mean', y = 'std', data = corr, ci = None, color = 'red')
plt.ylabel('Deviazione standard giornaliera', fontsize = 15)
plt.xlabel('Media giornaliera', fontsize = 15)
plt.show()

data = data.drop(['date'], axis = 1)

# test (log)
data['log_co'] = list(transform(data.co, 'log'))

train_log = data.log_co.loc[:'2005-02-12 23:00:00']
test_log = data.log_co.loc['2005-02-13 00:00:00':'2005-02-28 23:00:00']

# test (differentiation)
data_diff = list(difference(data.log_co))
data_diff.append(-0.035435)

data['diff_co'] = data_diff

train_diff = data.diff_co.loc[:'2005-02-13 00:00:00']
test_diff = data.diff_co.loc['2005-02-13 00:00:00':'2005-02-28 22:00:00']

kpss_test(data_diff)

"""### Seasonality"""

# Annual seasonality
plt.figure(figsize=(15, 6))
sns.lineplot(x = df['month'], y = df['co'], hue = df['year'], palette = 'Spectral')
plt.xlabel('Month', fontsize = 18)
plt.ylabel('Consumi di CO', fontsize = 18)
plt.legend(prop={'size': 16})
plt.show()

# Monthly seasonality
plt.figure(figsize=(15, 6))
sns.lineplot(x = df['day'], y = df['co'], hue = df['month'], ci=None, palette = 'mako_r' )
plt.xlabel('Day', fontsize = 16)
plt.ylabel('Consumi di CO', fontsize = 16)
plt.show()

# Weekly seasonality
plt.figure(figsize=(15, 6))
sns.lineplot(x=df['hour'], y=df['co'], hue=df['week'], ci = None, palette = 'mako_r',
             legend=False)
plt.xlabel('Hour', fontsize = 16)
plt.ylabel('Consumi di CO', fontsize = 16)
plt.show()

# Daily seasonality
plt.figure(figsize=(15, 6))
sns.lineplot(x = df['hour'], y = df['co'], hue = df['day'], ci = None, palette = 'mako_r', 
             legend=None)
plt.xlabel('Hour', fontsize = 16)
plt.ylabel('Consumi di CO', fontsize = 16)
plt.show()

"""## <font color='#e9c46a'><b>ARIMA

### Function
"""

def ARIMA_result(model, plot = False):
    """
    Funzione che permette, dato un modello ARIMA, di valutarne la bontà; attraverso
    l'analisi dei residui, la loro normalità attraverso normaltest e visualizzazione
    grafica. Segue la visualizzazione dei grafici acf e pacf dei residui.
    """
    residuals = model.resid

    k, p = normaltest(residuals)
    alpha = 1e-3
    if p < alpha:
        print("L'ipotesi nulla viene rigettata': i dati non sono distributi in modo normale")
    else:
        print("L'ipotesi nulla non viene rigettata: i dati sono distributi in modo normale")
    if plot == True:
      # Plot residui          
      sns.set_theme(style='darkgrid')
      fig = plt.figure(figsize=(12,8))
      
      ax = sns.histplot(residuals, bins=40, stat='density', color = 'c')
      
      mu, sigma = stats.norm.fit(residuals)
      xx = np.linspace(*ax.get_xlim())
      ax.plot(xx, stats.norm.pdf(xx, mu, sigma))
      plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
                loc='best')
      plt.ylabel('Frequency')
      plt.title('Residual distribution')
    
    # ACF e PACF sui residui
    fig = plt.figure(figsize=(12,8))
    ax1 = fig.add_subplot(211)
    fig = sm.graphics.tsa.plot_acf(model.resid, lags=200, ax=ax1, auto_ylims = True)
    ax2 = fig.add_subplot(212)
    fig = sm.graphics.tsa.plot_pacf(model.resid, lags=200, ax=ax2, auto_ylims = True,
                                    method='ywm')

def forecasting(model, plot = False, is_log = False, reg = False, reg_test = None):
  """
  Funzione che, dato un modello ARIMA, ne esegue il forecast sul periodo individuato
  come test set, ne calcola il MAPE e presenta una visualizzazione grafica.
  """
  start = '2005-02-13 00:00:00'
  end = '2005-02-28 23:00:00'

  # Forecasting
  if is_log == False:
    data['forecast'] = model.predict(start = start, end = end, dynamic= True) 

  elif reg == False:
    predictions = transform(model.predict(start = start, end = end, dynamic= True),
                            'exp')  
    temp = [None]*(len(data)-len(predictions))
    predictions = list(itertools.chain(temp, list(predictions)))
    data['forecast'] = predictions
  # Regressori
  else: 
    predictions = transform(model.predict(start = start, end = end, dynamic = True,
                                          exog = reg_test),
                            'exp')  
    temp = [None]*(len(data)-len(predictions))
    predictions = list(itertools.chain(temp, list(predictions)))
    data['forecast'] = predictions


  if plot == True:
    data[start:end][['co', 'forecast']].plot(figsize=(9, 4),
                                          color = ['steelblue','chocolate'])
    plt.title('Forecast delle rilevazioni di CO tra il 13-02-2005 e il 28-02-2005')
    plt.ylabel('CO values')
    plt.xlabel('Date Time')
    plt.grid(axis = 'both', color = 'gray', linewidth= 0.4)

  # Mape
  mape_index = mape(data[start:end]['co'],data[start:end]['forecast']) 

  return mape_index

"""### ARIMA """

fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(train.co, lags = 200, ax=ax1, auto_ylims = True)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(train.co, lags = 200, ax=ax2, auto_ylims = True, method='ywm')

"""From the PACF graph I observe how the first two lags are the most evident ones, I hypothesize $AR = 2$. I also see a pattern every 24 lags, which is most evident in the ACF graph.
Thus I define a very simple ARIMA model that does not take into account seasonality.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# model = ARIMA(data.log_co, order = (2,0,0)).fit()

print(model.summary())

ARIMA_result(model)

forecasting(model, plot = True, is_log = True)

"""Seasonality has not been covered having implemented a very simple ARIMA model; however, a pattern is evident every 24 lags. I proceed with the implementation of a SARIMA model that takes seasonality into account.

#### Altri test con modelli ARIMA
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# model = ARIMA(train_log, order = (2,0,1)).fit()

forecasting(model, plot = False, is_log = True)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# model = ARIMA(train_log, order = (3,0,1)).fit()

forecasting(model, plot = False, is_log = True)

"""Nessuno dei nuovi modelli testati risulta essere migliore di quello (2,0,0), per tale ragione quest'ultimo verrà tenuto come *base* per i successivi modelli SARIMA.

### SARIMA
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# sarima_model = sm.tsa.statespace.SARIMAX(train_log, order = (2,0,0),
#                                          trend = 'c', 
#                                          seasonal_order=(0,1,1,24)).fit()

print(sarima_model.summary())

ARIMA_result(sarima_model)

forecasting(sarima_model, plot = True, is_log = True)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# sarima_model_1 = sm.tsa.statespace.SARIMAX(train_log, order = (2,0,1),
#                                          trend = 'c', 
#                                          seasonal_order=(0,1,1,24)).fit()

print(sarima_model_1.summary())

ARIMA_result(sarima_model_1)

forecasting(sarima_model_1, plot = True, is_log = True)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# sarima_model_2 = sm.tsa.statespace.SARIMAX(train_log, order = (2,0,0),
#                                          trend = 'c', 
#                                          seasonal_order=(1,1,1,24)).fit()

print(sarima_model_2.summary())

ARIMA_result(sarima_model_2)

forecasting(sarima_model_2, plot = True, is_log = True)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# sarima_model_3 = sm.tsa.statespace.SARIMAX(train_log, order = (3,0,0),
#                                          trend = 'c', 
#                                          seasonal_order=(1,1,1,24)).fit()

print(sarima_model_3.summary())

ARIMA_result(sarima_model_3)

forecasting(sarima_model_3, plot = True, is_log = True)

"""### SARIMA with **regressors**"""

# Search for the best number of harmonics
num = np.arange(1,7)
mape_list = []
for i in num:
  sin_week = DeterministicProcess(data['log_co'].index,  period = 24*7, fourier = i)
  sin_year = DeterministicProcess(data['log_co'].index,  period = 24*365, fourier = i)
  regressor = sin_week.in_sample().merge(sin_year.in_sample(), left_index=True,
                                    right_index=True)
  
  reg_train = regressor[:len(train_log)]
  reg_test = regressor[len(train_log):]
  
  model = sm.tsa.statespace.SARIMAX(train_log,
                                  order=(2, 0, 0), seasonal_order=(1, 1, 1, 24),
                                  exog = reg_train, enforce_stationarity=False, 
                                  enforce_invertibility=False).fit()
  mape_list.append(forecasting(model, plot = True, is_log = True, reg = True))

"""The table below shows the MAPE values for each number of harmonics tested. The ideal number is **1**.

| _Harmonics's number_ | **MAPE** |
|--------------------|----------|
| 1                  | 11.30%   |
| 2                  | 12.96%   |
| 3                  | 13.58%   |
| 4                  | 17.04%   |
| 5                  | 23.13%   |
| 6                  | 45.60%   |
"""

# Best fourier
sin_week = DeterministicProcess(data['log_co'].index,  period = 24*7, fourier = 1)
sin_year = DeterministicProcess(data['log_co'].index,  period = 24*365, fourier = 1)
regressor = sin_week.in_sample().merge(sin_year.in_sample(), left_index=True,
                                  right_index=True)

reg_train = regressor[:len(train_log)]
reg_test = regressor[len(train_log):]

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# model_reg = sm.tsa.statespace.SARIMAX(train_log,
#                                   order=(2, 0, 0), seasonal_order=(1, 1, 1, 24),
#                                   exog = reg_train, enforce_stationarity=False, 
#                                   enforce_invertibility=False).fit()

forecasting(model_reg, plot = True, is_log = True, reg = True, 
            reg_test = reg_test)

# Combination of dummy regressors with harmonics
reg_combo_train = pd.merge(reg_train, df['holiday'], left_index=True,
                           right_index=True)
reg_combo_test = pd.merge(reg_test, df['holiday'], left_index=True,
                          right_index=True)

reg_combo_train1 = pd.merge(reg_train, df['weekend'], left_index=True,
                           right_index=True)
reg_combo_test1 = pd.merge(reg_test, df['weekend'], left_index=True,
                          right_index=True)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Model with holiday regressor and a Fourier transform
# model_holiday = sm.tsa.statespace.SARIMAX(train_log,
#                                   order=(2, 0, 0), seasonal_order=(1, 1, 1, 24),
#                                   exog = reg_combo_train, enforce_stationarity=False, 
#                                   enforce_invertibility=False).fit()

print(model_holiday.summary())

forecasting(model_holiday, plot = True, is_log = True, reg = True, 
            reg_test = reg_combo_test)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Model with weekend regressor and a Fourier transform
# model_weekend = sm.tsa.statespace.SARIMAX(train_log,
#                                   order=(2, 0, 0), seasonal_order=(1, 1, 1, 24),
#                                   exog = reg_combo_train1, enforce_stationarity=False, 
#                                   enforce_invertibility=False).fit()

print(model_weekend.summary())

forecasting(model_weekend, plot = True, is_log = True, reg = True, 
            reg_test = reg_combo_test1)

"""### Forecast in the future"""

start = '2005-03-01 00:00:00'
end = '2005-03-31 23:00:00'

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Prepare training set's regressors
# sin_week = DeterministicProcess(data.log_co.index,  period = 24*7, fourier = 1)
# sin_year = DeterministicProcess(data.log_co.index,  period = 24*365, fourier = 1)
# 
# regressor = sin_week.in_sample().merge(sin_year.in_sample(), left_index = True,
#                                   right_index = True)
# 
# reg_combo = pd.merge(regressor, df['holiday'], left_index = True, right_index = True)

# Best model
model_holiday = sm.tsa.statespace.SARIMAX(data.log_co,
                                  order=(2, 0, 0),
                                  seasonal_order=(1, 1, 1, 24),
                                  exog = reg_combo,
                                  enforce_stationarity = False, 
                                  enforce_invertibility = False).fit()

# Regressor for the future
sin_sett = DeterministicProcess(pd.date_range(start = start, end = end, freq='H'), 
                                 period = 168, fourier = 1)
sin_ann = DeterministicProcess(pd.date_range(start = start, end = end, freq='H'),
                                period = 24*365, fourier = 1)
regressor = sin_sett.in_sample().merge(sin_ann.in_sample(), left_index=True, 
                                       right_index=True)
# I only consider 744 regressors, because I have to predict 744 step in the future
reg_test = regressor[:len(fc_data)]
reg_combo_fut = pd.merge(reg_test, fc_data['holiday'], left_index=True, right_index=True)

forecast = transform(model_holiday.predict(start = start, end = end,
                                 reg = True, exog = reg_combo_fut), 'exp')

# Saving
fc_df['arima'] = forecast
fc_df.to_csv(PATH + 'final.csv', header = True, index = False)

# Forecast Plot
plt.rcParams["figure.figsize"] = (15,4)
y = (data.co[7700:len(data.co)])
ax = y.plot(label='Observed', figsize=(15, 4), lw = 1)
fc_df.arima.plot(ax=ax, label='Forecast', lw = 1, color = 'darkorange')
plt.grid()
ax.set_xlabel('Date')
ax.set_ylabel('CO levels')

plt.legend()
plt.title('Forecasting col modello UCM')
plt.show()

"""## <font color='f4a261'><b> UCM"""

# Model definition
def ucm_model(train, trend, cycle, season, is_log = True, reg_train = None, 
              reg_test = None):
  model = sm.tsa.UnobservedComponents(train,
                                      trend,
                                      stochastic_cycle = cycle,
                                      seasonal = 24, 
                                      freq_seasonal = season,
                                      exog = reg_train).fit()
  
  start = '2005-02-13 00:00:00'
  end = '2005-02-28 23:00:00'
  predictions = transform(model.predict(start = start, end = end, exog = reg_test),
                            'exp')

  temp = [None]*(len(data)-len(predictions))
  predictions = list(itertools.chain(temp, list(predictions)))
  data['forecast'] = predictions


  mape_index = mape(data[start:end]['co'],data[start:end]['forecast'])

  return mape_index

"""### Search for the best model"""

trends = ['ntrend', 'dconstant', 'rwalk', 'llevel', 'dtrend', 'lltrend',
          'rwdrift','rtrend', 'strend', 'lldtrend']

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # only trends
# mapes = [] 
# for trend in trends:
#   mapes.append(ucm_model(train_log, trend, False, None))

best_trends = ['dconstant', 'dtrend', 'lltrend','rwalk', 'llevel']

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Trend
# mapes = [] 
# for trend in best_trends:
#   mapes.append(ucm_model(train_log, trend, False, None))

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Trend + stochastic cycle
# mapes_cycle = []
# for trend in best_trends:
#   mapes_cycle.append(ucm_model(train_log, trend, True, None))

freq_seasonal = [{'period':24, 'harmonics':10}]

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Trends + trigonometric seasonality
# mapes_seas = []
# for trend in best_trends:
#   mapes_seas.append(ucm_model(train_log, trend, False, freq_seasonal))

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Trends + stochastic cycle + trigonometric seasonality
# mapes_cycle_st = []
# for trend in best_trends:
#   mapes_cycle_st.append(ucm_model(train_log, trend, True, freq_seasonal))

res = pd.DataFrame(columns = ['model', 'trend', 'cycle','seasonality','trend + cycle + seasonality'])
res['model'] = best_trends
res['trend'] = mapes
res['cycle'] = mapes_cycle
res['seasonality'] = mapes_seas
res['trend + cycle + seasonality'] = mapes_cycle_st

res

res.to_csv(PATH + 'ucm_best_trend.csv', header = True, index = False)

"""### UCM with regressors"""

sin_week = DeterministicProcess(data['log_co'].index,  period = 24, fourier = 1)
sin_year = DeterministicProcess(data['log_co'].index,  period = 24*365, fourier = 1)
regressor = sin_week.in_sample().merge(sin_year.in_sample(), left_index=True,
                                  right_index=True)

reg_train = regressor[:len(train_log)]
reg_test = regressor[:len(test_log)]

# Combination of dummy regressors with harmonics
reg_combo_train = pd.merge(reg_train, df['holiday'], left_index=True,
                           right_index=True)
reg_combo_test = pd.merge(reg_test, df['holiday'], left_index=True,
                          right_index=True)

reg_combo_train1 = pd.merge(reg_train, df['weekend'], left_index=True,
                           right_index=True)
reg_combo_test1 = pd.merge(reg_test, df['weekend'], left_index=True,
                          right_index=True)

freq_seasonal = [{'period':24, 'harmonics':10}] 
best_trends = ['dconstant', 'dtrend']

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# #Holiday
# mapes_holiday = []
# 
# for t in best_trends:
#   model = sm.tsa.UnobservedComponents(train_log,
#                                       t,
#                                       cycle = False,
#                                       seasonal = 24, 
#                                       freq_seasonal = freq_seasonal,
#                                       exog = reg_combo_train).fit()
#   # Forecasting + MAPE
#   start = '2005-02-13 00:00:00'
#   end = '2005-02-28 23:00:00'
#   predictions = transform(model.predict(start = start, end = end, exog = reg_combo_test),
#                               'exp')
#   temp = [None]*(len(data)-len(predictions))
#   predictions = list(itertools.chain(temp, list(predictions)))
#   data['forecast'] = predictions
#   mape_index = mape(data[start:end]['co'],data[start:end]['forecast'])
#   mapes_holiday.append(mape_index)

print(mapes_holiday)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Weedkend
# mapes_we = []
# 
# for t in best_trends:
#   model = sm.tsa.UnobservedComponents(train_log,
#                                       t,
#                                       cycle = False,
#                                       seasonal = 24, 
#                                       freq_seasonal = freq_seasonal,
#                                       exog = reg_combo_train1).fit()
#   # Forecasting + MAPE
#   start = '2005-02-13 00:00:00'
#   end = '2005-02-28 23:00:00'
#   predictions = transform(model.predict(start = start, end = end, exog = reg_combo_test1),
#                               'exp')
#   temp = [None]*(len(data)-len(predictions))
#   predictions = list(itertools.chain(temp, list(predictions)))
#   data['forecast'] = predictions
#   mape_index = mape(data[start:end]['co'],data[start:end]['forecast'])
#   mapes_we.append(mape_index)
#

print(mapes_we)

"""### **Forecast**"""

start = '2005-03-01 00:00:00'
end = '2005-03-31 23:00:00'

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Regressors for training set
# sin_week = DeterministicProcess(data.log_co.index,  period = 24, fourier = 1)
# sin_year = DeterministicProcess(data.log_co.index,  period = 24*365, fourier = 1)
# regressor = sin_week.in_sample().merge(sin_year.in_sample(), left_index=True,
#                                   right_index=True)
# 
# reg_combo = pd.merge(regressor, df['holiday'], left_index=True, right_index=True)
# 
# # Regressors for future
# sin_sett = DeterministicProcess(pd.date_range(start = start, end = end, freq='H'), 
#                                  period = 168, fourier = 1)
# sin_ann = DeterministicProcess(pd.date_range(start = start, end = end, freq='H'),
#                                 period = 24*365, fourier = 1)
# regressor = sin_sett.in_sample().merge(sin_ann.in_sample(), left_index=True, 
#                                        right_index=True)
# reg_test = regressor[:len(fc_data)]
# reg_combo_future = pd.merge(reg_test, fc_data['holiday'], left_index=True, right_index=True)

best_ucm = sm.tsa.UnobservedComponents(data.log_co, 'dtrend', seasonal = 24, 
                                       freq_seasonal = freq_seasonal,
                                       exog = reg_combo).fit()

predictions_ucm = transform(best_ucm.predict(start = start, end = end, exog = reg_combo_future),
                            'exp')

df = pd.read_csv(PATH + 'final.csv')
df['ucm'] = predictions_ucm
tdi = pd.DatetimeIndex(fc_df.dtime)
df = df.set_index(tdi, drop = True)

df.to_csv(PATH + 'final.csv', header = True, index = False)

# Forecast Plot
plt.rcParams['figure.figsize'] = (15,4)
y = (data.co[7700:len(data.co)])
ax = y.plot(label='Observed', figsize=(15, 4), lw = 1)
df.ucm.plot(ax=ax, label='Forecast', lw = 1, color = 'darkorange')
plt.grid()
ax.set_xlabel('Date')
ax.set_ylabel('CO levels')

plt.legend()
plt.title('Forecasting col modello UCM')
plt.show()

"""## <font color='e76f51'><b>Machine Learning

### Auxiliary Functions
There follows a series of auxiliary functions for processing and executing the models necessary for carrying out the task.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Re-load data
# data = pd.read_csv(PATH + 'train.csv') 
# data = date_time(data)
# data = interpolation(data)
# tdi = pd.DatetimeIndex(data.dtime)
# data = data.set_index(tdi, drop = True)        
# data = data.drop(labels = 'dtime', axis=1)
# data['log_co'] = list(transform(data.co, 'log'))

data_log = data.copy()
data_log = data_log.drop(labels = 'co', axis = 1)
train_log = data_log.loc[:'2005-02-12 23:00:00']
test_log = data_log.loc['2005-02-13 00:00:00':'2005-02-28 23:00:00']

def create_dataset(dataset, lag):
	"""
	Function necessary for processing data so that they can be
	used as input to recurrent neural networks
	
	:param dataset: the data to process
	:param int lag: number of lags
	"""
	dataX, dataY = [], []
	for i in range(len(dataset)-lag-1):
		a = dataset[i:(i+lag), 0]
		dataX.append(a)
		dataY.append(dataset[i + lag, 0])
	return np.array(dataX), np.array(dataY)

def prepare_data(train_sc, test_sc, lag = 24):
  # Data standardization
  x_train, y_train = create_dataset(train_sc, lag)
  x_test, y_test = create_dataset(test_sc, lag)
  
  # Data reshaping
  x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
  x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))

  return x_train, x_test, y_train, y_test

def fit_predict(model, x_train, x_test, y_train, y_test, out = False, ep = 50, bs = 32):
  # Fit
  model.fit(x_train, y_train, epochs = ep, batch_size = bs, verbose = 0)
  # Forecast
  y_pred_tr, y_test_ts = make_prediction(model, x_train, x_test, y_train, y_test)

  y_true = test_log.log_co[:-(lag+1)]
  y_pred = []
  for i in range(len(y_test_ts)):
    y_pred.append(y_test_ts[i][0])

  # MAPE
  print(mape(y_true, y_pred)) 
  if out == True:
    return y_true, y_pred

def make_prediction(model, x_train, x_test, y_train, y_test):
  # Forecast
  train_pred = model.predict(x_train)
  test_pred = model.predict(x_test)

  # Conversion of the forecast to the starting scale
  train_pred = sc.inverse_transform(train_pred)
  y_train = sc.inverse_transform([y_train])
  test_pred = sc.inverse_transform(test_pred)
  y_test = sc.inverse_transform([y_test])

  # Metrics
  trainScore = math.sqrt(mean_squared_error(y_train[0], train_pred[:,0]))
  print('Train Score: %.2f RMSE' % (trainScore))
  testScore = math.sqrt(mean_squared_error(y_test[0], test_pred[:,0]))
  print('Test Score: %.2f RMSE' % (testScore))

  return train_pred, test_pred

# Log data
lag = 1 
x_train_log = train_log.values
x_test_log = test_log.values
sc = MinMaxScaler(feature_range = (0, 1))
train_sc_log = sc.fit_transform(x_train_log)
test_sc_log = sc.fit_transform(x_test_log)

x_train_log, x_test_log, y_train_log, y_test_log = prepare_data(train_sc_log, test_sc_log, lag = 1)

print(train_log.values.shape)

"""### **LSTM**

#### Vanilla LSTM
It contains just one LSTM layer
"""

van_lstm = Sequential()
van_lstm.add(LSTM(4, activation='relu', input_shape=(1, lag)))
van_lstm.add(Dropout(0.4))
van_lstm.add(Dense(1))

van_lstm.compile(optimizer = 'adam', loss = 'mean_squared_error')

plot_model(van_lstm, show_shapes=True, show_layer_names=True)

fit_predict(van_lstm, x_train_log, x_test_log, y_train_log, y_test_log)

fit_predict(van_lstm, x_train_log, x_test_log, y_train_log, y_test_log)

"""#### Stacked LSTM"""

keras.backend.clear_session()
keras.backend.reset_uids()
model_s = Sequential()

model_s.add(LSTM(8, return_sequences = True, input_shape = (1, lag),
                   activation='relu',recurrent_activation= 'sigmoid',
                   recurrent_dropout=0, unroll=False, use_bias=True))
model_s.add(Dropout(0.5))
model_s.add(LSTM(4, activation='relu', return_sequences = False,
                   recurrent_activation= 'sigmoid', recurrent_dropout = 0, 
                   unroll=False, use_bias=True))
model_s.add(Dropout(0.5))

model_s.add(Dense(units = 1))

model_s.compile(optimizer = 'adam', loss = 'mean_squared_error')

plot_model(model_s, show_shapes=True, show_layer_names=True)

model_s.fit(x_train_log, y_train_log, epochs = 50, batch_size = 32, verbose = 0)
_, y_pred = make_prediction(model_s, x_train_log, x_test_log, y_train_log, y_test_log)
y_true = test_log.log_co[:-(lag+1)]
y_pred_s = []
for i in range(len(y_pred)):
  y_pred_s.append(y_pred[i][0])

fit_predict(model_s, x_train_log, x_test_log, y_train_log, y_test_log)

"""#### Bidirectional LSTM"""

keras.backend.clear_session()
keras.backend.reset_uids()

model_b = Sequential()

model_b.add(Bidirectional(LSTM(8, return_sequences = True, input_shape = (1, lag),
                   activation='relu',recurrent_activation= 'sigmoid',
                   recurrent_dropout=0, unroll=False, use_bias=True)))
model_b.add(Dropout(0.5))
model_b.add(Bidirectional(LSTM(4, activation='relu', return_sequences = False,
                   recurrent_activation = 'sigmoid', recurrent_dropout=0, 
                   unroll=False, use_bias=True)))
model_b.add(Dropout(0.5))
model_b.add(Dense(1))

model_b.compile(optimizer = 'adam', loss = 'mse')

plot_model(model_b, show_shapes=True, show_layer_names=True)

fit_predict(model_b, x_train_log, x_test_log, y_train_log, y_test_log)

"""### **GRU**"""

lag = 1
keras.backend.clear_session()
keras.backend.reset_uids()

model_gru = Sequential()

model_gru.add(GRU(units = 32, return_sequences = True, 
                  input_shape = (1, lag), activation='relu'))
model_gru.add(GRU(units = 8, activation='relu')) 
model_gru.add(Dense(units = 1)) 

model_gru.compile(optimizer = 'adam', loss = 'mse')

plot_model(model_gru, show_shapes=True, show_layer_names=True)

y_true, y_pred = fit_predict(model_gru, x_train_log, x_test_log, y_train_log, y_test_log)

y_true, y_pred = fit_predict(model_gru, x_train_log, x_test_log, y_train_log, y_test_log, out = True)

# Plot 
plt.plot(transform(list(y_true), 'exp'), label='Valori reali')
plt.plot(transform(list(y_pred),'exp'), label='Valori predetti')
plt.legend()
plt.ylabel('Consumi di CO')
plt.title('Confronto tra valori attuali e valori predetti con GRU')
plt.show()

"""#### Forecast in the future with GRU"""

# Prepare data
lag = 1 
data_log = data.copy()
data_log = data_log.drop(labels = 'log_co', axis = 1)

x_train = data_log.values
x_train, y_train, = prepare_data(x_train, lag = 1)

# Build model
lag = 1
keras.backend.clear_session()
keras.backend.reset_uids()

model_gru = Sequential()

model_gru.add(GRU(units = 32, return_sequences = True, 
                  input_shape = (1, lag), activation = 'relu'))
model_gru.add(GRU(units = 8, activation = 'relu')) 
model_gru.add(Dense(units = 1)) 

model_gru.compile(optimizer = 'adam', loss = 'mse',run_eagerly=True)

# Fit model
model_gru.fit(x_train, y_train, epochs = 50, batch_size = 32, verbose = 0)

y_pred = []
day = 0
while day < 7:
  x_train = x_train_temp
  y_train = y_train_temp

  if day !=0:
    # Retrain the model on the new data
    model_gru.fit(x_train_temp, y_train_temp, epochs = 10, batch_size = 32, verbose = 0)

  # Prediction on the last 120 instances
  y_true = x_train[-120:]
  pred = model_gru.predict(y_true)
  y_pred.extend(pred.flatten())

  temp = pd.DataFrame(columns = ['co'])
  temp.co = list(data_log.co) + y_pred

  x_train_temp = temp.values
  x_train_temp, y_train_temp, = prepare_data(x_train, lag = 1)

  day+=1

# Forecast Plot
plt.rcParams['figure.figsize'] = (15,4)
plt.grid()
temp = pd.DataFrame()
true = list(itertools.chain(list(data.co), [None]* len(y_pred)))
pred = list(itertools.chain([None]*len(data.co),list(y_pred)))
temp['true'] = true
temp['pred'] = pred
plt.plot(temp.true[7700:], lw = 1)
plt.plot(temp.pred, color = 'darkorange', lw = 1)
plt.ylabel('CO levels')
plt.title('Forecasting con GRU')